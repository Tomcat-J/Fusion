# 多原型分类头深度分析报告

## 一、文件概览

| 文件 | 状态 | 主要问题 |
|------|------|----------|
| `head.py` | ⚠️ 可运行但有理论问题 | Contrastive Loss 设计错误 |
| `Head1.py` | ⚠️ 可运行但有理论问题 | 同上 + 返回格式不一致 |
| `optimalhead.py` | ❌ 无法运行 | 多个未定义变量 |
| `head_fixed.py` | ✅ 修复版本 | 新创建 |

---

## 二、理论问题分析

### 2.1 ❌ Contrastive Loss 设计错误（严重）

**当前实现 (`head.py` 第 107-116 行):**
```python
def compute_contrastive_loss(self, prototypes, valid_mask):
    valid_protos = prototypes[valid_mask]
    protos_norm = F.normalize(valid_protos, p=2, dim=-1)
    sim_matrix = torch.mm(protos_norm, protos_norm.t())
    mask = ~torch.eye(n, device=sim_matrix.device).bool()
    contrastive_loss = sim_matrix[mask].mean()  # 最小化所有非对角线相似度
```

**问题分析:**

假设有 7 个类别，每个类别 5 个原型，共 35 个原型：
- 类别 A: P1, P2, P3, P4, P5
- 类别 B: P6, P7, P8, P9, P10
- ...

当前 Contrastive Loss 会：
1. 推开 P1 和 P2（同类）❌
2. 推开 P1 和 P6（异类）✅
3. 推开 P2 和 P3（同类）❌

**与 Diversity Loss 的冲突:**
- Diversity Loss: 惩罚同类原型相似度 > 0.5
- Contrastive Loss: 惩罚所有原型相似度 > 0

这两个损失互相矛盾！Diversity Loss 允许同类原型有一定相似度，但 Contrastive Loss 要求所有原型都远离。

**修复方案:**
```python
def compute_contrastive_loss_fixed(self, prototypes, valid_mask, proto_indices):
    # 构建同类掩码
    same_class_mask = torch.zeros(n, n, dtype=torch.bool)
    idx = 0
    for start, end in proto_indices:
        num_protos = end - start
        same_class_mask[idx:idx+num_protos, idx:idx+num_protos] = True
        idx += num_protos
    
    # 只惩罚异类原型的相似度
    diff_class_mask = ~same_class_mask & ~torch.eye(n).bool()
    contrastive_loss = sim_matrix[diff_class_mask].mean()
```

---

### 2.2 ⚠️ 门控融合逻辑不完整

**当前实现:**
```python
gate_input = torch.cat([cross_out, self_out, local_out], dim=-1)  # 3个输入
gate = self.gate(gate_input)  # 输出 emb_dim
updated_proto = gate * local_out + (1 - gate) * proto  # 只用了2个！
```

**问题:** 
- 输入用了 3 个特征 (cross_out, self_out, local_out)
- 输出只融合了 local_out 和原始 proto
- cross_out 和 self_out 的信息被丢弃

**修复方案:**
```python
# 三路 softmax 门控
gate_weights = F.softmax(self.gate(gate_input), dim=-1)  # (B, N, 3)
updated_proto = (
    gate_weights[..., 0:1] * cross_out +
    gate_weights[..., 1:2] * self_out +
    gate_weights[..., 2:3] * light_out
)
```

---

### 2.3 ⚠️ "Local Self-Attention" 命名误导

**当前实现:**
```python
self.local_self_attn = nn.MultiheadAttention(emb_dim, num_heads // 2, ...)
```

**问题:** 减少 attention heads 数量不会产生"局部"注意力效果。真正的 Local Attention 需要限制感受野（如 sliding window）。

**建议:** 改名为 `lightweight_self_attn`，更准确描述其功能。

---

### 2.4 ⚠️ 动量更新逻辑非标准

**当前实现:**
```python
def momentum_update(self, proto_idx, new_value):
    self.momentum_cache[proto_idx] = m * cache + (1-m) * new_value
    self.cache_count[proto_idx] += 1

def apply_momentum_updates(self):
    self.prototypes.data[idx] = cache / count  # 混合了 EMA 和平均
```

**问题:** 先做 EMA 累积，再除以 count，语义不清。

**标准 EMA:**
```python
def momentum_update_fixed(self, proto_idx, new_value):
    self.prototypes.data[proto_idx] = (
        m * self.prototypes.data[proto_idx] + (1-m) * new_value
    )
```

---

## 三、工程问题分析

### 3.1 ❌ `optimalhead.py` 无法运行

```python
# 第 67 行 - self.margin 未定义
sep_loss += torch.clamp(self.margin - nearest_other, min=0).mean()

# 第 70-74 行 - embeds 未定义
norms = embeds.norm(dim=1, p=2, keepdim=True) + 1e-6

# 第 77 行 - margin_loss, diversity_loss, self.diversity_scale 未定义
total_loss = margin_loss + self.diversity_scale * diversity_loss + ...
```

### 3.2 ⚠️ 返回格式不一致

| 模式 | `head.py` | `Head1.py` |
|------|-----------|------------|
| 训练 | `(logits, loss)` | `(logits, loss, loss_dict)` |
| 推理 | `(logits, 0.0)` | `(logits, similarities, uncertainty)` |

**问题:** 调用方需要根据 `model.training` 解析返回值，容易出错。

**修复:** 统一返回 `(logits, loss, loss_dict)`，推理时 `loss=0, loss_dict=None`。

---

## 四、损失函数对比

| 特性 | `head.py` | `optimalhead.py` | `head_fixed.py` |
|------|-----------|------------------|-----------------|
| 距离度量 | 1 - cosine | cdist (欧氏) | 可选 |
| Cluster Loss | ✅ 类别平衡 | ✅ 类别平衡 | ✅ 类别平衡 |
| Separation Loss | ✅ Margin | ✅ Margin | ✅ Margin |
| Diversity Loss | ✅ 同类原型 | ❌ 缺失 | ✅ 同类原型 |
| Contrastive Loss | ❌ 推开所有 | ❌ 推开所有 | ✅ 只推开异类 |
| 可运行 | ✅ | ❌ | ✅ |

---

## 五、长尾数据的特殊考虑

你的数据是长尾分布，这对损失函数设计有重要影响：

### 5.1 当前的类别平衡策略
```python
weight = 1.0 / torch.sqrt(mask.sum().float() + 1e-6)
```

这是合理的，但可以考虑更激进的策略：
```python
# 方案1: 对数加权
weight = 1.0 / torch.log(mask.sum().float() + 2)

# 方案2: 有效样本数加权 (Class-Balanced Loss)
beta = 0.9999
effective_num = 1.0 - beta ** mask.sum().float()
weight = (1.0 - beta) / effective_num
```

### 5.2 原型数量与长尾的关系

对于长尾数据，可以考虑：
- 头部类别（样本多）: 更多原型，捕捉多样性
- 尾部类别（样本少）: 较少原型，避免过拟合

```python
# 动态原型数量
if class_sample_count > 1000:
    init_prototypes = 7
elif class_sample_count > 100:
    init_prototypes = 5
else:
    init_prototypes = 3
```

---

## 六、修复文件说明

已创建 `projects/mae_lite/head_fixed.py`，包含：

1. `BalancedPrototypeLossFixed` - 修复后的损失函数
2. `MixedAttentionBlockFixed` - 修复后的混合注意力
3. `DynamicPrototypeManagerFixed` - 修复后的原型管理器
4. `HeadFixed` - 完整的修复版分类头

**使用方法:**
```python
from head_fixed import HeadFixed

model = HeadFixed(
    num_classes=7,
    emb_dim=384,
    num_heads=8,
    img_feat_dim=384,
    num_prototypes=5,
    max_prototypes=10,
    use_euclidean=False  # 可选欧氏距离
)

# 训练
logits, loss, loss_dict = model(pool_features, img_features, labels)

# 推理
logits, _, _ = model(pool_features, img_features)
```

---

## 七、建议的下一步

1. **替换 `head.py`**: 用 `head_fixed.py` 替换或合并
2. **删除 `optimalhead.py`**: 该文件无法运行
3. **实验验证**: 对比修复前后的性能
4. **长尾优化**: 根据数据分布调整原型数量和损失权重
