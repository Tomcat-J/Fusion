# --------------------------------------------------------
# Copyright (c) 2022 Megvii, Inc. and its affiliates. All Rights Reserved.
# --------------------------------------------------------
import os
import torch
from timm.models import create_model
from projects.eval_tools.util.lr_decay import create_optimizer, LayerDecayValueAssigner

from mae_lite.exps.timm_imagenet_exp import Exp as BaseExp, Model
from mae_lite.layers import build_lr_scheduler
from loguru import logger
from mae_lite.models.models_vit import *
from projects.eval_tools.models_vit_rpe import *
from projects.mae_lite.models_mae import *


class Exp(BaseExp):
    def __init__(self, batch_size, max_epoch=100):
        super(Exp, self).__init__(batch_size, max_epoch)
        # MAE
        self.layer_decay = 1.0
        self.global_pool = True

        # dataset & model
        self.dataset = "MyDataSet"
        self.img_size = 224

        #optimizer
        self.opt = "adamw"
        self.opt_eps = 1e-8
        self.opt_betas = (0.9, 0.999)
        self.momentum = 0.9
        self.weight_decay = 0.01
        self.clip_grad = None


        #schedule
        self.sched = "warmcos_scale"
        self.basic_lr_per_img = 1e-4 / batch_size
        self.warmup_lr = 0.0
        self.min_lr = 1e-6
        self.warmup_epochs = 1

        # augmentation & regularization
        self.color_jitter = 0.4
        self.aa = "rand-m9-mstd0.5-inc1"
        self.reprob = 0.25
        self.remode = "pixel"
        self.recount = 1
        # self.resplit = False
        self.mixup = 0.8
        self.cutmix = 1.0
        self.smoothing = 0.1
        self.train_interpolation = "bicubic"
        # self.drop = 0.0
        # self.drop_connect = None
        self.drop_path = 0.2
        self.attn_drop_rate = 0.0
        # self.drop_block = None
        self.encoder_arch = "HiFuse_Small"

        # self.num_workers = 10
        self.weights_prefix = ""
        # self.print_interval = 10
        # self.enable_tensorboard = True
        self.pretrain_exp_name = "HiFuse_Small_1e-5-0.05"
        self.save_folder_prefix = "ft_HiFuse_Small"

    @property
    def exp_name(self):
        assert self.pretrain_exp_name is not None, "Please provide a valid 'pretrain_exp_name'!"
        return os.path.join(self.pretrain_exp_name, "{}eval".format(self.save_folder_prefix))

    @exp_name.setter
    def exp_name(self, value):
        # exp_name is generated by exp.pretrain_exp_name and exp.save_folder_prefix.
        # Directly setting does not affect.
        pass

    def get_model(self):
        if "model" not in self.__dict__:
            # 检查是否使用增强版模型
            use_enhanced_model = getattr(self, 'use_enhanced_model', False)
            enhancement_reduction = getattr(self, 'enhancement_reduction', 0.0625)
            
            if use_enhanced_model:
                # 使用增强版模型 (main_model_enhanced)
                # 需要修改encoder_arch为增强版
                enhanced_arch = self.encoder_arch + "_Enhanced" if not self.encoder_arch.endswith("_Enhanced") else self.encoder_arch
                
                encoder = create_model(
                    enhanced_arch,
                    pretrained=self.pretrained,
                    num_classes=self.num_classes,
                    drop_rate=self.drop,
                    drop_path_rate=self.drop_path,
                    attn_drop_rate=self.attn_drop_rate,
                    drop_block_rate=self.drop_block,
                    global_pool=self.global_pool,
                    enhancement_reduction=enhancement_reduction,
                )
                logger.info(f"Using enhanced model: {enhanced_arch} (reduction={enhancement_reduction})")
            else:
                # 使用原始模型 (main_model)
                encoder = create_model(
                    self.encoder_arch,
                    pretrained=self.pretrained,
                    num_classes=self.num_classes,
                    drop_rate=self.drop,
                    drop_path_rate=self.drop_path,
                    attn_drop_rate=self.attn_drop_rate,
                    drop_block_rate=self.drop_block,
                    global_pool=self.global_pool,
                )
                logger.info(f"Using original model: {self.encoder_arch}")
            
            self.model = Model(self, encoder)
        return self.model
        return self.model

    def get_optimizer(self):
        if "optimizer" not in self.__dict__:
            if "lr" not in self.__dict__:
                self.lr = self.basic_lr_per_img * self.batch_size

            # 检查是否使用增强版模型和差异化学习率
            use_enhanced_model = getattr(self, 'use_enhanced_model', False)
            
            if use_enhanced_model:
                # 使用差异化学习率
                from projects.mae_lite.mhf_enhancement import get_parameter_groups
                
                backbone_lr_scale = getattr(self, 'backbone_lr_scale', 0.1)
                mhf_lr_scale = getattr(self, 'mhf_lr_scale', 0.5)
                enhancement_lr_scale = getattr(self, 'enhancement_lr_scale', 1.0)
                
                logger.info(f"Using differential learning rates:")
                logger.info(f"  backbone: {backbone_lr_scale}x, mhf: {mhf_lr_scale}x, enhancement: {enhancement_lr_scale}x")
                
                param_groups = get_parameter_groups(
                    self.get_model().model,
                    backbone_lr_scale=backbone_lr_scale,
                    mhf_lr_scale=mhf_lr_scale,
                    enhancement_lr_scale=enhancement_lr_scale,
                    base_lr=self.lr
                )
                
                self.optimizer = torch.optim.AdamW(
                    param_groups,
                    lr=self.lr,
                    betas=self.opt_betas,
                    eps=self.opt_eps,
                    weight_decay=self.weight_decay
                )
            else:
                # 原始优化器逻辑
                num_layers = 12
                if self.layer_decay < 1.0:
                    assigner = LayerDecayValueAssigner(
                        list(self.layer_decay ** (num_layers + 1 - i) for i in range(num_layers + 2))
                    )
                else:
                    assigner = None

                if assigner is not None:
                    logger.info("Assigned values = %s" % str(assigner.values))

                skip_weight_decay_list = self.get_model().model.no_weight_decay()
                logger.info("Skip weight decay list: {}".format(skip_weight_decay_list))

                self.optimizer = create_optimizer(
                    self,
                    self.get_model().model,
                    skip_list=skip_weight_decay_list,
                    get_num_layer=assigner.get_layer_id if assigner is not None else None,
                    get_layer_scale=assigner.get_scale if assigner is not None else None,
                )
        return self.optimizer

    def get_lr_scheduler(self):
        if "lr" not in self.__dict__:
            self.lr = self.basic_lr_per_img * self.batch_size
        if "warmup_lr" not in self.__dict__:
            self.warmup_lr = self.warmup_lr_per_img * self.batch_size
        if "min_lr" not in self.__dict__:
            self.min_lr = self.min_lr_per_img * self.batch_size

        optimizer = self.get_optimizer()
        iters_per_epoch = len(self.get_data_loader()["train"])
        scheduler = build_lr_scheduler(
            self.sched,
            optimizer,
            self.lr,
            total_steps=iters_per_epoch * self.max_epoch,
            warmup_steps=iters_per_epoch * self.warmup_epochs,
            warmup_lr_start=self.warmup_lr,
            end_lr=self.min_lr,
        )
        return scheduler

    def set_current_state(self, current_step, ckpt_path=None):
        if current_step == 0:
            # load pretrain ckpt
            if ckpt_path is None:
                assert self.pretrain_exp_name is not None, "Please provide a valid 'pretrain_exp_name'!"
                ckpt_path = os.path.join(self.output_dir, self.pretrain_exp_name, "last_epoch_ckpt.checkpoints.tar")
            logger.info("Load pretrained checkpoints from {}.".format(ckpt_path))
            msg = self.set_model_weights(ckpt_path, map_location="cpu")
            logger.info("Model params {} are not loaded".format(msg.missing_keys))
            logger.info("State-dict params {} are not used".format(msg.unexpected_keys))

    # def set_model_weights(self, ckpt_path, map_location="cpu"):
    #     if not os.path.isfile(ckpt_path):
    #         from torch.nn.modules.module import _IncompatibleKeys
    #
    #         logger.info("No checkpoints found! Training from scratch!")
    #         return _IncompatibleKeys(missing_keys=None, unexpected_keys=None)
    #     ckpt = torch.load(ckpt_path, map_location="cpu")
    #     weights_prefix = self.weights_prefix
    #     if not weights_prefix:
    #         state_dict = {k: v for k, v in ckpt["model"].items()}
    #     else:
    #         if weights_prefix and not weights_prefix.endswith("."):
    #             weights_prefix += "."
    #         if all(key.startswith("module.") for key in ckpt["model"].keys()):
    #             weights_prefix = "module." + weights_prefix
    #         state_dict = {k.replace(weights_prefix, "model."): v for k, v in ckpt["model"].items()}
    #     msg = self.get_model().load_state_dict(state_dict, strict=False)
    #     return msg


    def set_model_weights(self, ckpt_path, map_location="cpu"):
        BLACK_LIST = ("head", )
        # BLACK_LIST = ("classifier",)
        def _match(key):
            return any([k in key for k in BLACK_LIST])

        if not os.path.isfile(ckpt_path):
            from torch.nn.modules.module import _IncompatibleKeys

            logger.info("No checkpoints found! Training from scratch!")
            return _IncompatibleKeys(missing_keys=None, unexpected_keys=None)
        ckpt = torch.load(ckpt_path, map_location="cpu")
        weights_prefix = self.weights_prefix
        if not weights_prefix:
            state_dict = {"model." + k: v for k, v in ckpt["state_dict"].items() if not _match(k)}
            # state_dict = {"model." + k: v for k, v in ckpt.items() if not _match(k)}
        else:
            if weights_prefix and not weights_prefix.endswith("."):
                weights_prefix += "."
            if all(key.startswith("module.") for key in ckpt["model"].keys()):
                weights_prefix = "module." + weights_prefix
            state_dict = {k.replace(weights_prefix, "model."): v for k, v in ckpt["model"].items() if not _match(k)}
        msg = self.get_model().load_state_dict(state_dict, strict=False)
        return msg


if __name__ == "__main__":
    exp = Exp(2)
    exp.update({"pretrain_exp_name": "mae_lite/mae_lite_exp"})
    print(exp.exp_name)
    model = exp.get_model()
    print(model)
    loader = exp.get_data_loader()
    opt = exp.get_optimizer()
    scheduler = exp.get_lr_scheduler()
    ckpt_path = '/home/backup/lh/KD/outputs/HiFuse_Small/HiFuse_Small_eval_200e/ft_eval/best_ckpt.checkpoints.tar'
    ckpt = torch.load(ckpt_path, map_location="cpu")

    state_dict = {"model." + k: v for k, v in ckpt["model"].items()}
    # print(state_dict)
    # weight = exp.set_model_weights(ckpt_path)
    msg = exp.get_model().load_state_dict(state_dict, strict=False)
    print(msg)
